# 测试左移&右移
常规流程: `需求 - 开发 - 测试 - 上线`. 从这个流程可以看出测试这个角色才参与流程是靠后的,因为依赖于开发交付的成果.

但是这种模式会存在一些问题, 
1. 工作量不均衡, 测试在开发编码阶段工作量较轻, 到测试这个阶段又过重, 开发则反之
2. 测试介入靠后, 开发成果存在缺陷, 越后期修复成本越高. 测试人员精力疲于修复简单的缺陷,而不能专注于深层次测试中且有较大可能性导致上线延期

## 破局
通过测试左移右移, 让测试有更多的主动权

### 测试左移
本质: 越早发现不合理的地方, 出问题的概率越低

目的: 
1. 提高质量上线: 通过一系列措施, 使问题在起步阶段就得到解决
2. 提高质量下限: 通过一系列措施, 使开发交付的成果底线得到了保障
3. 缩小测试周期: 交付的成果质量高, 那么测试环节的时间也会相应的降低

核心:
全员质量服务意识, 都为产品质量服务. 测试团队是有限的, 质量的参与人员不仅仅局限于测试人员, 只有通过赋能质量意识, 使团队成员都具备质量意识, 才能有效提高质量

措施:
- 有哪些活动可以提高质量上限（举例）？

1. 健康的项目流程（合理并且严格遵守的项目流程）
2. 合理的需求分析（评估需求的质量，分析需求的合理性以及完整性）
3. 出色的系统架构
4. 完整的系统设计（评估设计的质量，分析需求的合理性以及完整性）
5. 进行研发标准的定义
6. 更早的测试分析（先于开发完成需求的分析，做好各种评审的准备）
7. 尽早的测试执行（提早参与测试执行，在集成前就发现一些问题）

- 有哪些活动可以提高质量下限（举例）？

1. 健康的测试流程
2. 优秀的测试用例
3. 合理的测试计划
4. 合适的自动化
5. 适当的探索式测试
6. 充分利用静态代码扫描
7. 开发自测（TDD、BDD，测试提供更好的用例、技术支持）
8. 团队质量意识的培养

落地措施:
1. 质量文化的赋能
2. 质量规范梳理
3. 项目进度(里程碑, 项目周期)
4. 环节交付输出 (需求理解,单元测试, 上线时间, 线上问题)


### 测试右移
目的: 及时发现,及时反馈

核心: 围绕问题反馈、发现、定位、监控展开，参与人员则不仅仅局限于运维人员

措施: 
1. 通过接入`prometheus` + `granfana` 实现数据监控
    1. 服务器性能(cpu, 内存, i/o, 网络)
    2. 业务指标(注册量, 进件量, 通过量, 拒绝量, 放款量, 还款量, 短信发送量, 三方验证量)
    3. 风控指标
2. 接入阿里云日志告警(邮件, 短信)

# 测试理论
## 测试计划
1. 测试范围
2. 测试策略
    1. 明确”产品目标“
    2. 进行”风险分析“
    3. 适配”产品研发流程“
3. 测试资源
    1. 人力资源
    2. 环境资源
    3. 设备资源

# DevOps&CICD
见 devops文件夹

# 工程效能
1. 持续发布能力，它包括发布频率和发布前置时间，也就是从代码提交到功能上线花费的时间。
2. 需求响应周期，它包括交付周期时间和开发周期时间，交付周期时间指的是从确认用户提出的需求开始，到需求上线所经历的平均时长。开发周期时间指的是从开发团队理解需求开始，到需求可以上线所经历的平均时长。
3. 交付吞吐率，指的是单位时间内交付需求的数量；实际研发中，可以给出标准化的story/task定义，保证每个项目组的考核通用性。
4. 交付过程质量，它包含两个细分的指标，分别是开发过程中bug的创建和修复时间分布，以及bug库存; 当然, 也可以加一些譬如reopen率这样的指标。
5. 对外交付质量，它包含两个细分的指标，分别是单位时间的线上故障数，以及故障平均解决时长。

# 管理

# 大厂前沿
AI测试
1. 手淘AIOps实战-消息全链路异常检测
2. 从0到1，机器学习产品的精益质量

# jira
流程: 开始, 待确认, 修复中, stash, 重新打开, 待验证, 测试环境修复, 关闭, 完成

模块: 项目, 问题类型, 优先级, 概要, 经办人, 版本, 模块, 环境, 附件, 详情, 报告人

# 流程
- 瀑布式: 是一种线形的、顺序的软件开发模型
1. 特点: 顺序性, 依赖性, 具备里程碑特征, 基于文档驱动, 严格的阶段评审
2. 优点: 有利于大型软件开发过程人员的组织和管理
3. 缺点: 不够灵活, 周期长

# 用例设计
1. 等价类: 标准, 有效等价类, 无效等价类
2. 边界值: 针对条件限制的边缘进行测试
3. 错误推测法: 基于经验和直觉推测程序中所有可能存在的各种错误,从而有针对性的设计测试用例的方法
4. 因果图: 对输入条件的组合情况、输入条件之间的相互制约关系进行分析
5. 场景设计法: 通过事件流的形式测试集成状态

# 质量模型:
1. 功能性: 适合性, 准确性, 安全性
2. 可靠性: 成熟性, 容错性, 易恢复性
3. 易用性: 易理解性, 易学性, 易操作性
4. 效率: 时间, 资源
5. 维护性: 易分析性, 稳定性, 易测试性
6. 可移植性: 易安装, 易替换

# 质量影响要素
技术, 流程, 阻止

# 测试用例
用例编号

用例标题

功能模块名称

前置条件

输入数据

操作步骤

预期结果

优先级

执行结果

编写人

执行人

# django
## 请求流程
`get_wsgi_application() --> WSGIHandler() --> self.load_middleware() --> __call__() --> self.get_response() --> confurl.dispatch --> view --> Reponse`

# 算法
1. 父子数组, 合并去重
`reduce(lambda x,y: set(x) | set(y), array)`

# 自定义nodeid
参数:
1. rename: 判断插件是否开启
2. skip: 指定需要skip的nodeid
3. skip-json: 指定需要skip的json文件

工作流:
1. `pytest_collection_modifyitems`
- 判断是否开启使用
- 提取正则结果
- 修改nodeid name

2. pytest_configure
- 判断是否开启skip和skip-json
- 将skip和skip-json合并

3. pytest_report_teststatus
- 判断是否为call阶段
- 判断状态是否为失败
- 将nodeid头部存储

4. pytest_runtest_setup
- 判断当前用例是否需要skip
- 执行skip

# 失败截图
参数:
1. screenshot: 开关
2. screenshot_path: 截图地址

1. pytest_configure
- 如果两个参数都为on, 则归档历史截图

2. pytest_runtest_makereport
- 查找driver实例
- 根据模式 执行截图
    1. 保存到指定地址
    2. 保存到`项目/screenshot`
    3. 无保存,仅附加到


# api框架
参数:
- -E: 环境
- -P: 项目
- -S: sheet
- --mark: sheet文件


1. pytest_configure
- 初始化配置, 将配置文件加载到上下文中

2. pytest_xdist_auto_num_workers
- 将并发数量调整为sheet数

3. pytest_generate_tests
- 读取excel
- 用例参数化

事件
1. 强制等待
2. 前置sql
3. 动态请求
4. 响应断言
5. 提取数据
6. 数据库断言
7. 后置sql
8. 归档

# ui框架
参数
- -P: 项目
- -E: 环境
- -S: 服务器
- -M: 模式
- -A: app包位置
- --device: 指定设备

1. pytest_configure
- 加载配置

2. pytest_collection_modifyitems
- 根据模式选择运行用例
- 将未选中用例添加到pytest_deselected中

3. pytest_runtest_makereport
- 在web模式下,发送cookie,用于在zalenium中声明运行结果

4. pytest_runtest_call
- 在web模式下,发送cookie,用于在zalenium中做注释

api用例: 500  sheet: 40+
平台用例: 700 场景100+
ui用例: 100 